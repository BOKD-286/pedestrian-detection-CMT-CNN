{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize filters and outputs using the network architecture proposed by Krizhevsky et al. for ImageNet and implemented in `caffe`.\n",
    "\n",
    "(This page follows DeCAF visualizations originally by Yangqing Jia.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import required modules, set plotting parameters, and run `./scripts/download_model_binary.py models/bvlc_reference_caffenet` to get the pretrained CaffeNet model if it hasn't already been fetched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Make sure that caffe is on the python path:\n",
    "caffe_root = '../'  # this file is expected to be in {caffe_root}/examples\n",
    "import sys\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "import caffe\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "import os\n",
    "if not os.path.isfile(caffe_root + 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'):\n",
    "    print(\"Downloading pre-trained CaffeNet model...\")\n",
    "    !../scripts/download_model_binary.py ../models/bvlc_reference_caffenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Caffe to CPU mode, load the net in the test phase for inference, and configure input preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caffe.set_mode_cpu()\n",
    "net = caffe.Net(caffe_root + 'models/bvlc_reference_caffenet/deploy.prototxt',\n",
    "                caffe_root + 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel',\n",
    "                caffe.TEST)\n",
    "\n",
    "# input preprocessing: 'data' is the name of the input blob == net.inputs[0]\n",
    "transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n",
    "transformer.set_transpose('data', (2,0,1))\n",
    "transformer.set_mean('data', np.load(caffe_root + 'python/caffe/imagenet/ilsvrc_2012_mean.npy').mean(1).mean(1)) # mean pixel\n",
    "transformer.set_raw_scale('data', 255)  # the reference model operates on images in [0,255] range instead of [0,1]\n",
    "transformer.set_channel_swap('data', (2,1,0))  # the reference model has channels in BGR order instead of RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify the image by reshaping the net for the single input then doing the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class is #281.\n"
     ]
    }
   ],
   "source": [
    "net.blobs['data'].reshape(1,3,227,227)\n",
    "net.blobs['data'].data[...] = transformer.preprocess('data', caffe.io.load_image(caffe_root + 'examples/images/cat.jpg'))\n",
    "out = net.forward()\n",
    "print(\"Predicted class is #{}.\".format(out['prob'].argmax()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layer features and their shapes (1 is the batch size, corresponding to the single input image in this example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', (1, 3, 227, 227)),\n",
       " ('conv1', (1, 96, 55, 55)),\n",
       " ('pool1', (1, 96, 27, 27)),\n",
       " ('norm1', (1, 96, 27, 27)),\n",
       " ('conv2', (1, 256, 27, 27)),\n",
       " ('pool2', (1, 256, 13, 13)),\n",
       " ('norm2', (1, 256, 13, 13)),\n",
       " ('conv3', (1, 384, 13, 13)),\n",
       " ('conv4', (1, 384, 13, 13)),\n",
       " ('conv5', (1, 256, 13, 13)),\n",
       " ('pool5', (1, 256, 6, 6)),\n",
       " ('fc6', (1, 4096)),\n",
       " ('fc7', (1, 4096)),\n",
       " ('fc8', (1, 1000)),\n",
       " ('prob', (1, 1000))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, v.data.shape) for k, v in net.blobs.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters and their shapes. The parameters are `net.params['name'][0]` while biases are `net.params['name'][1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conv1', (96, 3, 11, 11)),\n",
       " ('conv2', (256, 48, 5, 5)),\n",
       " ('conv3', (384, 256, 3, 3)),\n",
       " ('conv4', (384, 192, 3, 3)),\n",
       " ('conv5', (256, 192, 3, 3)),\n",
   